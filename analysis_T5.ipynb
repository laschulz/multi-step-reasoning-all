{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUVw4Rpm9Get",
        "outputId": "316faba0-d5f0-49b5-adb6-a2cbc9bba4d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TzF-iXWGSEY",
        "outputId": "71f4202b-24f9-4dff-ac58-787e93ace7b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug78CNVh9RTi",
        "outputId": "2b1a277f-8370-4aff-f1d8-801aa0548150"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqvpDT8BOUFo",
        "outputId": "eb5b96bd-16e7-499e-c348-5577134a0568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.22.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.4.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.15.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80IhEbnXOX8L",
        "outputId": "6a59196f-223c-4f9f-e6dd-bad61dfff539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.4)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (2.3.0)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install flask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SGhDuxKTV-t",
        "outputId": "b53af89b-89aa-410a-b133-b5eb77528639"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bert_score in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.0.1+cu118)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.5.3)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.30.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.65.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (23.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2022.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->bert_score) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->bert_score) (16.0.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.15.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.3.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=3.0.0->bert_score) (2023.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert_score) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install bert_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9nL-DmavPp_c"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from evaluate import load\n",
        "from flask import request, jsonify\n",
        "import jsonlines\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-oEsF_73-9nk"
      },
      "outputs": [],
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(\"laschulz/t5-large\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"laschulz/t5-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "66mhxeu-KSXC"
      },
      "outputs": [],
      "source": [
        "def extract_jsonl(file_path):\n",
        "    data = []\n",
        "    with jsonlines.open(file_path) as reader:\n",
        "        for item in reader:\n",
        "            data.append(item)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "database = extract_jsonl(\"./test_socratic.jsonl\")"
      ],
      "metadata": {
        "id": "Dp1yMEijBudS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xsAK6nETN1zF"
      },
      "outputs": [],
      "source": [
        "def parser_expectedAnswer(expected_answer):\n",
        "    temp = str(expected_answer).split('\\n') #str might be redundant\n",
        "    temp = [line.split('**')[0] for line in temp]\n",
        "    if '###' in temp[-1]:\n",
        "        temp.pop()\n",
        "    return temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uqr8FYjfuCDJ"
      },
      "outputs": [],
      "source": [
        "def parser_output(model_output):\n",
        "    parsed_output = model_output.split('?')\n",
        "    #parsed_output = [entry.strip() + '?' if entry.strip() != \"\" else \"\" for entry in model_output.split('?')]\n",
        "    for i in range(len(parsed_output)):\n",
        "        entry = parsed_output[i]\n",
        "        stripped = entry.strip()\n",
        "        if (stripped != \"\"):\n",
        "            parsed_output[i] = stripped + '?'\n",
        "        else:\n",
        "            parsed_output.pop(i)\n",
        "    return parsed_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pB44Nyr3Megu"
      },
      "outputs": [],
      "source": [
        "bertscore = load(\"bertscore\")\n",
        "def bert_score(references, predictions): \n",
        "    results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
        "    return results['f1']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_database(database):\n",
        "    all_middle = 0\n",
        "    all_lower = 0\n",
        "    some_lower = 0\n",
        "    last_lower = 0\n",
        "    all_upper = 0\n",
        "    some_upper = 0\n",
        "    first_upper = 0\n",
        "    both = 0\n",
        "\n",
        "    for item in database:\n",
        "        input_text = item[\"question\"]\n",
        "        expected_answer = item[\"answer\"]\n",
        "        expected_answer = parser_expectedAnswer(expected_answer)\n",
        "        inputs = tokenizer([input_text], return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        # Check if a GPU is available\n",
        "        if torch.cuda.is_available():\n",
        "            # Set the device to GPU\n",
        "            device = torch.device(\"cuda\")\n",
        "        else:\n",
        "            # If GPU is not available, fall back to CPU\n",
        "            device = torch.device(\"cpu\")\n",
        "\n",
        "        # Move the model and inputs to the GPU\n",
        "        model.to(device)\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        output_sequence = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            do_sample=False,\n",
        "        )\n",
        "        decoded_output = tokenizer.batch_decode(output_sequence, skip_special_tokens=True)\n",
        "\n",
        "        parsed_output = parser_output(decoded_output[0])\n",
        "\n",
        "        if (len(parsed_output) == len(expected_answer)):\n",
        "            bertScore = bert_score(expected_answer,parsed_output)\n",
        "\n",
        "            if all(0.8925 < score < 0.9925 for score in bertScore):\n",
        "                all_middle += 1\n",
        "            if all(score < 0.8925 for score in bertScore):\n",
        "                all_lower += 1\n",
        "            elif any(score < 0.8925 for score in bertScore):\n",
        "                some_lower += 1\n",
        "            if all(score > 0.9925 for score in bertScore):\n",
        "                all_upper += 1\n",
        "            elif any(score > 0.9925 for score in bertScore):\n",
        "                some_upper += 1\n",
        "            if bertScore[(len(bertScore)-1)] < 0.8925:\n",
        "                last_lower += 1\n",
        "            if bertScore[(len(bertScore)-1)] > 0.9925:\n",
        "                first_upper += 1\n",
        "\n",
        "            if any(score > 0.9925 for score in bertScore) and any(score < 0.8925 for score in bertScore):\n",
        "                both += 1\n",
        "\n",
        "            \n",
        "    return (all_middle, all_lower, some_lower, last_lower, all_upper, some_upper, first_upper, both)\n",
        "  \n",
        "\n",
        "all_middle, all_lower, some_lower, last_lower, all_upper, some_upper, first_upper, both = process_database(database)"
      ],
      "metadata": {
        "id": "Jt3GMVwzRwRx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "af9909eb-4b4e-4efc-8d94-fbe74f4696fc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-89feb7251801>\u001b[0m in \u001b[0;36m<cell line: 63>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mall_middle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msome_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_upper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msome_upper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_upper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_database\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-89feb7251801>\u001b[0m in \u001b[0;36mprocess_database\u001b[0;34m(database)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         output_sequence = model.generate(\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m             \u001b[0;31m# 11. run greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m             return self.greedy_search(\n\u001b[0m\u001b[1;32m   1523\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2338\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2339\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2340\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2341\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1720\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1721\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1088\u001b[0m                 )\n\u001b[1;32m   1089\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1091\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    721\u001b[0m                 \u001b[0mquery_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m             cross_attention_outputs = self.layer[1](\n\u001b[0m\u001b[1;32m    724\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkey_value_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    632\u001b[0m     ):\n\u001b[1;32m    633\u001b[0m         \u001b[0mnormed_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m         attention_output = self.EncDecAttention(\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mnormed_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_middle, all_lower, some_lower, last_lower, all_upper, some_upper, first_upper, both)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9ARIY9xPFY_",
        "outputId": "91fdf3c4-b620-4962-e9f0-bb08442fbaa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "191 2 69 66 33 119 81 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "d_1cKKD29RK8"
      },
      "outputs": [],
      "source": [
        "def process_database(database):\n",
        "    results = []\n",
        "    raw_results = []\n",
        "    length_is_not_the_same = 0\n",
        "    length_is_same = 0\n",
        "    over_generation = 0\n",
        "    question_missing = 0\n",
        "    for item in database:\n",
        "        input_text = item[\"question\"]\n",
        "        expected_answer = item[\"answer\"]\n",
        "        expected_answer = parser_expectedAnswer(expected_answer)\n",
        "        inputs = tokenizer([input_text], return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        # Check if a GPU is available\n",
        "        if torch.cuda.is_available():\n",
        "            # Set the device to GPU\n",
        "            device = torch.device(\"cuda\")\n",
        "        else:\n",
        "            # If GPU is not available, fall back to CPU\n",
        "            device = torch.device(\"cpu\")\n",
        "\n",
        "        # Move the model and inputs to the GPU\n",
        "        model.to(device)\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        output_sequence = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            do_sample=False,\n",
        "        )\n",
        "        decoded_output = tokenizer.batch_decode(output_sequence, skip_special_tokens=True)\n",
        "\n",
        "        parsed_output = parser_output(decoded_output[0])\n",
        "\n",
        "        if (len(parsed_output) != len(expected_answer)):\n",
        "            length_is_not_the_same += 1\n",
        "            if (len(parsed_output) > len(expected_answer)):\n",
        "                over_generation += 1\n",
        "            else:\n",
        "                question_missing += 1\n",
        "        else:\n",
        "            length_is_same += 1\n",
        "            print(\"same: \", length_is_same)\n",
        "            bertScore = bert_score(expected_answer,parsed_output)\n",
        "            result_dict = {\n",
        "                \"bert_score\": bertScore,\n",
        "                \"model_output\": parsed_output,\n",
        "                \"expected_answer\": expected_answer\n",
        "            }\n",
        "            for i in range(len(bertScore)):\n",
        "                score = bertScore[i]\n",
        "                if (bertScore[i] > 0.9925 ):\n",
        "                  break;\n",
        "\n",
        "                if (bertScore[i] < 0.8925):\n",
        "                  break\n",
        "                if (i == len(bertScore)-1):\n",
        "                    raw_results_dict = { \n",
        "                        \"question\": item[\"question\"],\n",
        "                        \"answer\": item[\"answer\"]\n",
        "                    }\n",
        "                    raw_results.append(raw_results_dict)\n",
        "                    print(\"parsed_output: \", parsed_output)\n",
        "                    print(\"expected_outut: \", expected_answer)\n",
        "\n",
        "            results.append(result_dict)\n",
        "            \n",
        "    return (results, raw_results, length_is_not_the_same, over_generation, question_missing)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_database(database):\n",
        "    raw_results_good = []\n",
        "    raw_results_bad = []\n",
        "    length_is_not_the_same = 0\n",
        "    length_is_same = 0\n",
        "    for item in database:\n",
        "        input_text = item[\"question\"]\n",
        "        expected_answer = item[\"answer\"]\n",
        "        expected_answer = parser_expectedAnswer(expected_answer)\n",
        "        inputs = tokenizer([input_text], return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        # Check if a GPU is available\n",
        "        if torch.cuda.is_available():\n",
        "            # Set the device to GPU\n",
        "            device = torch.device(\"cuda\")\n",
        "        else:\n",
        "            # If GPU is not available, fall back to CPU\n",
        "            device = torch.device(\"cpu\")\n",
        "\n",
        "        # Move the model and inputs to the GPU\n",
        "        model.to(device)\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        output_sequence = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            do_sample=False,\n",
        "        )\n",
        "        decoded_output = tokenizer.batch_decode(output_sequence, skip_special_tokens=True)\n",
        "\n",
        "        parsed_output = parser_output(decoded_output[0])\n",
        "\n",
        "        if (len(parsed_output) != len(expected_answer)):\n",
        "            length_is_not_the_same += 1\n",
        " \n",
        "        else:\n",
        "            length_is_same += 1\n",
        "            print(\"same: \", length_is_same)\n",
        "            bertScore = bert_score(expected_answer,parsed_output)\n",
        "            result_dict = {\n",
        "                \"bert_score\": bertScore,\n",
        "                \"model_output\": parsed_output,\n",
        "                \"expected_answer\": expected_answer\n",
        "            }\n",
        "            for i in range(len(bertScore)):\n",
        "                score = bertScore[i]\n",
        "                #looking at cases where there is a correct answer\n",
        "                if (bertScore[i] > 0.995):\n",
        "                  raw_results_dict = { \n",
        "                        \"question\": item[\"question\"],\n",
        "                        \"answer\": item[\"answer\"]\n",
        "                  }\n",
        "                  raw_results_good.append(raw_results_dict)\n",
        "                  break;\n",
        "            \n",
        "            for i in range(len(bertScore)):\n",
        "                score = bertScore[i]\n",
        "                #looking at cases where there is a wrong answer\n",
        "                if (bertScore[i] < 0.90):\n",
        "                  raw_results_dict = { \n",
        "                        \"question\": item[\"question\"],\n",
        "                        \"answer\": item[\"answer\"]\n",
        "                  }\n",
        "                  raw_results_bad.append(raw_results_dict)\n",
        "                  break;\n",
        "            \n",
        "    return (raw_results_good, raw_results_bad, length_is_not_the_same)"
      ],
      "metadata": {
        "id": "V-WELQyi7vfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpTA7oOIH6Pb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc50fa36-a3ee-419b-c09c-6c74238f665c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "same:  1\n",
            "parsed_output:  ['How many eggs does Janet eat every day?', 'How many eggs does Janet have left?']\n",
            "expected_outut:  ['How many eggs does Janet sell? ', \"How much does Janet make at the farmers' market? \"]\n",
            "same:  2\n",
            "parsed_output:  ['How many white fibers does the robe need?', 'How many bolts in total does it?']\n",
            "expected_outut:  ['How many bolts of white fiber does it take? ', 'How many bolts in total does it take? ']\n",
            "same:  3\n",
            "parsed_output:  ['How many meters does James run in each sprint?', 'How many meters does James run in total?']\n",
            "expected_outut:  ['How many sprints does James run in a week? ', 'How many meters does James run in a week? ']\n",
            "same:  4\n",
            "same:  5\n",
            "same:  6\n",
            "same:  7\n",
            "parsed_output:  ['How much did Mishka spend on shorts?', 'How much did Mishka spend on?']\n",
            "expected_outut:  ['How many dollars did Mishka spend on all the clothing items? ', 'How many dollars did Mishka spend on all the clothing items? ']\n",
            "same:  8\n",
            "same:  9\n",
            "same:  10\n",
            "parsed_output:  ['How much do the two pairs of heels cost?', 'How much do the boots cost?', 'How much?']\n",
            "expected_outut:  ['How much does the second pair of heels cost? ', 'How much do the heels together cost? ', 'How much do the boots cost? ']\n",
            "same:  11\n",
            "parsed_output:  ['How many hours does John spend walking each dog?', 'How many hours does John spend taking care of?']\n",
            "expected_outut:  ['How many hours does John spend taking care of dogs? ', 'How many hours does John spend taking care of dogs? ']\n",
            "same:  12\n",
            "same:  13\n",
            "same:  14\n",
            "same:  15\n",
            "same:  16\n",
            "same:  17\n",
            "same:  18\n",
            "same:  19\n",
            "parsed_output:  ['How much money does Peter have?', 'How many times can Peter go to the movies?']\n",
            "expected_outut:  ['How much does it cost him? ', 'How many times can he go to the movies? ']\n",
            "same:  20\n",
            "parsed_output:  ['How much was the delivery fee?', 'How much was the delivery fee?', 'What was the final price?']\n",
            "expected_outut:  ['How much did they charge him in fees? ', 'How much did they charge him in fees? ', \"What was the final price of Stephen's groceries? \"]\n",
            "same:  21\n",
            "parsed_output:  ['How much did the jeweler charge?', 'How much did Janet pay?', 'How much did Jane?']\n",
            "expected_outut:  ['How much did Janet pay? ', 'How much did Janet pay for insurance? ', 'How much did Janet pay? ']\n",
            "same:  22\n",
            "parsed_output:  ['How many gallons of gas does Sophia have left?', 'How many miles can Sophia drive on?']\n",
            "expected_outut:  ['How many miles does Sophia get per gallon? ', 'How many miles can Sophia drive on a single tank? ']\n",
            "same:  23\n",
            "same:  24\n",
            "same:  25\n",
            "same:  26\n",
            "same:  27\n",
            "same:  28\n",
            "parsed_output:  ['How much does the grooming cost after the discount?', 'How much does the grooming cost after?']\n",
            "expected_outut:  ['How much did the groomer give as a discount? ', 'How much did the grooming cost? ']\n",
            "same:  29\n",
            "parsed_output:  ['How much does Alex weigh?', 'How much do Grace and Alex weigh together?']\n",
            "expected_outut:  ['How many pounds does Alex weigh? ', 'What are their combined weights in pounds? ']\n",
            "same:  30\n",
            "same:  31\n",
            "same:  32\n",
            "same:  33\n",
            "same:  34\n",
            "parsed_output:  ['How many cars were in the traffic jam after the first 15 minutes?', 'How many cars drove through?']\n",
            "expected_outut:  ['How many cars drove through the traffic jam in the first 15 minutes? ', 'How many cars drove through the traffic jam in the first 15 minutes? ']\n",
            "same:  35\n",
            "parsed_output:  ['How many miles is the city?', 'How many minutes will it take for the fog bank to cover?']\n",
            "expected_outut:  ['How many intervals will it take to cover the city? ', 'How many minutes will it take for the fog bank to cover the whole city? ']\n",
            "same:  36\n",
            "same:  37\n",
            "same:  38\n",
            "parsed_output:  ['How many teachers are in the classroom?', 'How many times is the whiteboard cleaned in a?']\n",
            "expected_outut:  ['How many lessons are there in a day? ', 'How many times is the whiteboard cleaned in a day? ']\n",
            "same:  39\n",
            "parsed_output:  ['How many flowers does Ryan plant in 15 days?', 'How many flowers does Ryan have after 15 days?']\n",
            "expected_outut:  ['How many flowers does Ryan plant in total? ', 'How many flowers does Ryan have in his garden? ']\n",
            "same:  40\n",
            "same:  41\n",
            "same:  42\n",
            "same:  43\n",
            "same:  44\n",
            "same:  45\n",
            "same:  46\n",
            "parsed_output:  ['How many spoons did Julia have after tasting the stew?', 'How many spoons were in the?']\n",
            "expected_outut:  ['How many spoons were in the package that Julia bought? ', 'How many spoons were in the package that Julia bought? ']\n",
            "same:  47\n",
            "same:  48\n",
            "same:  49\n",
            "parsed_output:  [\"How much was Joseph's expenditure in June?\", \"How much was Joseph's total expenditure for?\"]\n",
            "expected_outut:  ['How much was his expenditure in June? ', 'How much was his total expenditure for those two months? ']\n",
            "same:  50\n",
            "same:  51\n",
            "same:  52\n",
            "same:  53\n",
            "parsed_output:  ['How many cars are automatic?', 'How many cars are manual?', 'How many cars are semi-automat?']\n",
            "expected_outut:  ['How many cars are automatic or manual? ', 'How many cars are semi-automatic? ', 'What percentage of the cars are semi-automatic? ']\n",
            "same:  54\n",
            "parsed_output:  ['How much money does Jordan earn from his part-time job?', 'How much money does Jordan earn?']\n",
            "expected_outut:  ['How many hours does Jordan play video games? ', 'How much money would Jordan earn in one week if he spent his video game time working instead? ']\n",
            "same:  55\n",
            "same:  56\n",
            "same:  57\n",
            "parsed_output:  ['How many bananas do the monkeys need every month?', 'How many bananas do the go?']\n",
            "expected_outut:  ['How many bananas do the prime apes need every month? ', 'How many bananas do the prime apes need in 2 months? ']\n",
            "same:  58\n",
            "parsed_output:  ['How much did the pens cost?', 'How much did the notebooks cost?', 'How much did?']\n",
            "expected_outut:  ['How much did Raphael spend on the pens? ', 'How much did Raphael spend on the notebooks? ', 'How much did Raphael spend on everything? ']\n",
            "same:  59\n",
            "same:  60\n",
            "parsed_output:  ['How deep was the tank on Tuesday?', 'How deep is the tank on Wednesday?']\n",
            "expected_outut:  ['How many feet of water were in the tank on Tuesday? ', 'How many feet of water were in the tank on Wednesday? ']\n",
            "same:  61\n",
            "parsed_output:  ['How much do the action figures cost?', 'How much do the red cars cost?', 'How much do?']\n",
            "expected_outut:  ['How much are three action figures worth? ', 'How much are the red cars worth? ', 'How much are all toys worth? ']\n",
            "same:  62\n",
            "same:  63\n",
            "parsed_output:  ['How much money does Elvira spend?', 'How much money will Elvira have left for?']\n",
            "expected_outut:  ['How much does the computer equipment cost? ', 'How much money will she have left for her clothing? ']\n",
            "same:  64\n",
            "parsed_output:  ['How many eggs does Sandra get from her neighbor?', 'How many times does Sandra have to babys?']\n",
            "expected_outut:  ['How many eggs does Sandra need? ', 'How many times does Sandra have to babysit? ']\n",
            "same:  65\n",
            "same:  66\n",
            "parsed_output:  ['How many Asians were on the Chinese team?', 'How many boys were on the Chinese team?']\n",
            "expected_outut:  ['How many Chinese were there? ', 'How many girls were there? ']\n",
            "same:  67\n",
            "parsed_output:  ['How much did Lloyd earn for the first week?', 'How much did Lloyd earn for the second week?']\n",
            "expected_outut:  ['How many hours did Lloyd tutor? ', 'How much did Lloyd earn for the first two weeks? ']\n",
            "same:  68\n",
            "same:  69\n",
            "parsed_output:  ['How many red roses did Sandra order?', 'How many pink calla lilies did?']\n",
            "expected_outut:  ['How many white carnations did Sandra order? ', 'How many red roses did Sandra order? ']\n",
            "same:  70\n",
            "same:  71\n",
            "parsed_output:  ['How many peaches does John collect in 3 hours?', 'How many peaches does John collect in?']\n",
            "expected_outut:  ['How many minutes does John collect peaches? ', 'How many peaches does John collect? ']\n",
            "same:  72\n",
            "same:  73\n",
            "parsed_output:  ['How many crayons does Violetta need?', 'How much money does Violetta have?']\n",
            "expected_outut:  ['How much will Violetta pay for the crayons? ', 'How much change will Violetta get? ']\n",
            "same:  74\n",
            "parsed_output:  ['How much do a pencil and eraser cost together?', 'How much do 8 pens cost?']\n",
            "expected_outut:  ['How much does a pen cost? ', 'How much will 8 pens cost? ']\n",
            "same:  75\n",
            "parsed_output:  ['How many pounds of carrots are to be distributed?', 'How many pounds of carrots will not?']\n",
            "expected_outut:  ['How many pounds of carrots do the restaurants need? ', 'How many pounds of carrots will not be used? ']\n",
            "same:  76\n",
            "parsed_output:  ['How many people are there in Soda?', 'How many people are there in Soda?']\n",
            "expected_outut:  ['How many adults are there? ', 'How many children are there? ']\n",
            "same:  77\n",
            "parsed_output:  ['How many tons of grapes does Josie grow?', 'How many barrels of wine does Jo?']\n",
            "expected_outut:  ['How many tons of grapes does her farm produce per year? ', 'How many barrels of wine does her farm produce per year? ']\n",
            "same:  78\n",
            "same:  79\n",
            "same:  80\n",
            "parsed_output:  ['How much did the company spend on advertising for the first year?', 'How much did the company spend?']\n",
            "expected_outut:  ['How much is spent on advertising in the second year? ', \"What's the total amount the company spent on advertising for the two years? \"]\n",
            "same:  81\n",
            "parsed_output:  ['How many days will Andrew spend traveling by bus?', 'How many days will Andrew spend traveling by car?']\n",
            "expected_outut:  ['How many days will Andrew travel by car? ', 'How many days will Andrew travel? ']\n",
            "same:  82\n",
            "same:  83\n",
            "parsed_output:  ['How many bugs were there?', 'How many insects were there?']\n",
            "expected_outut:  ['How many bugs are there? ', 'How many insects are there? ']\n",
            "same:  84\n",
            "parsed_output:  ['How much money did Johnny have after putting in the $10?', 'How much money did Johnny have?']\n",
            "expected_outut:  ['How much money did Johnny invest? ', 'How much money did Johnny triple? ']\n",
            "same:  85\n",
            "same:  86\n",
            "same:  87\n",
            "parsed_output:  ['How many books does Dolly and Pandora read together?', 'How many books will Dolly and Pandora?']\n",
            "expected_outut:  ['How many books are there in total? ', 'How many books do Dolly and Pandora read together? ']\n",
            "same:  88\n",
            "same:  89\n",
            "same:  90\n",
            "same:  91\n",
            "parsed_output:  ['How many apps did Travis delete?', 'How many apps did Travis download?']\n",
            "expected_outut:  ['How many apps did Travis have after deleting the ones he didn ', 'How many apps does Travis have now? ']\n",
            "same:  92\n",
            "parsed_output:  ['How many floors did Bill ride?', 'How many floors is Bill on now?']\n",
            "expected_outut:  ['How many floors are in the building? ', 'How many floors are in the building? ']\n",
            "same:  93\n",
            "parsed_output:  ['How much does Nancy owe for the books?', 'How much does Nancy owe in?']\n",
            "expected_outut:  ['How much is the late fee? ', 'How much does Nancy have to pay total? ']\n",
            "same:  94\n",
            "same:  95\n",
            "parsed_output:  ['How many marbles does Maddison have?', 'How many marbles does Maddi?']\n",
            "expected_outut:  ['How many marbles does Maddison have from her boxes? ', 'How many marbles does Maddison have now? ']\n",
            "same:  96\n",
            "same:  97\n",
            "parsed_output:  ['How much does Samantha have?', 'How much does Daisy have?', 'How much do all three girls have?']\n",
            "expected_outut:  ['How much money does Samantha have? ', 'How much money does Daisy have? ', 'How much money do all three girls have combined? ']\n",
            "same:  98\n",
            "same:  99\n",
            "same:  100\n",
            "same:  101\n",
            "parsed_output:  ['How old will Mico and Marco be in 10 years?', 'What will be the sum of their?']\n",
            "expected_outut:  ['How many years will it be in 10 years? ', 'What will be the sum of their ages in 10 years? ']\n",
            "same:  102\n",
            "parsed_output:  ['How many guests are there?', 'How many people are there?', 'How much does each pizza cost?']\n",
            "expected_outut:  ['How many people are buying pizza? ', 'How many pizzas does Maddy need to buy? ', 'How much money will Maddy spend? ']\n",
            "same:  103\n",
            "same:  104\n",
            "same:  105\n",
            "same:  106\n",
            "parsed_output:  ['How many minutes did Peter exercise on Sunday?', 'How many minutes did Peter exercise on Monday and Sunday?']\n",
            "expected_outut:  ['How many minutes did Peter exercise on Sunday and Monday? ', 'How many minutes does Peter have to exercise on Tuesday? ']\n",
            "same:  107\n",
            "parsed_output:  ['How many pages are in a 32-page tabloid?', 'How many pieces of paper would?']\n",
            "expected_outut:  ['How many pages does each piece of paper print? ', 'How many pieces of paper would be used in a 32-page tabloid? ']\n",
            "same:  108\n",
            "parsed_output:  ['How much did Annika spend on food and snacks?', 'How much did Annika spend on rides?']\n",
            "expected_outut:  ['How much did she spend on food? ', 'How much money is left? ']\n",
            "same:  109\n",
            "parsed_output:  ['How many people did John beat?', 'How many people did John lose to?']\n",
            "expected_outut:  ['How many people does John beat? ', 'How many people does John lose to? ']\n",
            "same:  110\n",
            "same:  111\n",
            "same:  112\n",
            "same:  113\n",
            "parsed_output:  ['How many records did the 5 people bring in?', 'How many new records did the 5 people get?']\n",
            "expected_outut:  ['How many old records can people bring in? ', 'How many old records did the 5 people bring in? ']\n",
            "same:  114\n",
            "same:  115\n",
            "same:  116\n",
            "same:  117\n",
            "same:  118\n",
            "parsed_output:  ['How many students are in each of the other groups?', 'How many students are in the first two?']\n",
            "expected_outut:  ['How many students are in each group? ', 'How many students are in the smallest group? ']\n",
            "same:  119\n",
            "parsed_output:  ['How many balls does Josh add to his act each week?', 'How many balls does Josh add to?']\n",
            "expected_outut:  ['How many balls does Josh start with? ', 'How many balls does Josh have left? ']\n",
            "same:  120\n",
            "parsed_output:  ['How many baby outfits did Laurel receive at her baby shower?', 'How many baby outfit?']\n",
            "expected_outut:  ['How many baby outfits did Laurel receive at the baby shower? ', 'How many baby outfits does Laurel have? ']\n",
            "same:  121\n",
            "same:  122\n",
            "same:  123\n",
            "same:  124\n",
            "parsed_output:  ['How much money did Andrea have after buying the sweater?', 'How much money did Andrea save?']\n",
            "expected_outut:  ['How much did Andrea spend? ', 'How much did Andrea save? ']\n",
            "same:  125\n",
            "same:  126\n",
            "parsed_output:  ['How many fourth grade girls were at Small Tree School on Friday?', 'How many fourth grade boys were?']\n",
            "expected_outut:  ['How many fourth grade boys are at Small Tree  School? ', 'How many fourth grade boys were at Small Tree  School on Friday? ']\n",
            "same:  127\n",
            "parsed_output:  ['How many boys are there?', 'How many kids are there?']\n",
            "expected_outut:  ['How many boys are in the park? ', 'How many kids are in the park? ']\n",
            "same:  128\n",
            "parsed_output:  ['How many points does Jane need to score?', 'How many points does Jane need to score on the?']\n",
            "expected_outut:  ['How many points did Jane score on the first two tests? ', 'How many points must Jane score on the third test to pass? ']\n",
            "same:  129\n",
            "parsed_output:  ['How many points does Erin have now?', 'How many points did Sara have before?', 'How many?']\n",
            "expected_outut:  ['How many points did Erin have after? ', 'How many points did Erin have after? ', 'How many points did Erin have before? ']\n",
            "same:  130\n",
            "parsed_output:  ['How much did Ethan save on materials?', 'How much did Ethan save in total?']\n",
            "expected_outut:  ['How much will a DIY save? ', 'How much will a hand-painted wallpaper made at home cost? ']\n",
            "same:  131\n",
            "parsed_output:  ['How many pieces of equipment were faulty?', 'How much money was spent on functioning pieces of equipment?']\n",
            "expected_outut:  ['How much were the faulty pieces of equipment bought for? ', 'How much was the functioning equipment bought for? ']\n",
            "same:  132\n",
            "parsed_output:  ['How many sheep does Mary have?', 'How much milk does Mary collect every day?']\n",
            "expected_outut:  ['How many sheep are in the first half? ', 'How much milk does Mary get every day? ']\n",
            "same:  133\n",
            "parsed_output:  ['How many pieces of bread did Mrs. Sherman feed to the children?', 'How many pieces of bread?']\n",
            "expected_outut:  ['How many rolls were left after feeding her children? ', 'How many pieces of rolls did she feed to the chickens? ']\n",
            "same:  134\n",
            "parsed_output:  ['How many cookies did Rachel and Janet have together?', \"How many cookies did Rachel's brother?\"]\n",
            "expected_outut:  ['How many cookies does Rachel have? ', 'How many cookies were left after her brother’s feast? ']\n",
            "same:  135\n",
            "parsed_output:  ['How many boys are in the choir?', 'How many girls are in the choir?', 'How many people?']\n",
            "expected_outut:  ['How many people are performing? ', 'How many people are performing? ', 'How many people are singing? ']\n",
            "same:  136\n",
            "parsed_output:  ['How many points did Wilson receive?', 'What is his current math grade?']\n",
            "expected_outut:  ['How many points did Wilson receive on his 5 math tests? ', 'How many points did Wilson receive on his 5 math tests? ']\n",
            "same:  137\n",
            "same:  138\n",
            "same:  139\n",
            "parsed_output:  ['How much do the ride tickets cost?', 'How much money does David save?', 'How much money does?']\n",
            "expected_outut:  ['How much does each ride cost? ', 'How much do all 9 rides cost? ', 'How much does David save? ']\n",
            "same:  140\n",
            "parsed_output:  ['How many gifts does Monica have?', 'How many inches of ribbon can Monica use for each gift bow?']\n",
            "expected_outut:  ['How many gifts does Monica have to wrap? ', 'How many inches of ribbon can Monica use for each gift? ']\n",
            "same:  141\n",
            "same:  142\n",
            "parsed_output:  ['How much money does Mark make each day?', 'How many days does Mark have to save his money?']\n",
            "expected_outut:  ['How much money does Mike need to save? ', 'How many days does Mike have to save? ']\n",
            "same:  143\n",
            "same:  144\n",
            "same:  145\n",
            "parsed_output:  ['How many cookies does Shannon eat in a day?', 'How many cookies does Shannon need to?']\n",
            "expected_outut:  ['How many cookies does Shannon need to eat in 30 days? ', 'How many dozens of cookies does Shannon need to make? ']\n",
            "same:  146\n",
            "parsed_output:  ['How many feet did the comb fall?', 'How many feet must Oliver climb to reach the elevation?']\n",
            "expected_outut:  [\"How many feet was Stanley's comb? \", \"How many feet must Oliver climb to reach the elevation of Stanley's comb? \"]\n",
            "same:  147\n",
            "parsed_output:  ['How many lollipops did Ray have left?', 'How many lollipops did Ray share?']\n",
            "expected_outut:  ['How many lollipops did Ray share? ', 'How many lollipops did each of his friends receive? ']\n"
          ]
        }
      ],
      "source": [
        "#adjust this line to define which process_database function is being used\n",
        "results, raw_results, length_is_not_the_same, over_generation, question_missing = process_database(database)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INz8JpLdVyIm"
      },
      "outputs": [],
      "source": [
        "print(length_is_not_the_same)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jsonlines\n",
        "\n",
        "def save_results_to_jsonl(results, filename):\n",
        "    with jsonlines.open(filename, 'w') as writer:\n",
        "        for result in results:\n",
        "            writer.write(result)"
      ],
      "metadata": {
        "id": "pBNxEunNvsHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_results_to_jsonl(results_analysis, 'results.jsonl')"
      ],
      "metadata": {
        "id": "N4SoBW6tclX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_results_to_jsonl(raw_results_good, 'filtered_test_good.jsonl')"
      ],
      "metadata": {
        "id": "L4bcZ7cTcniU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_results_to_jsonl(raw_results_bad, 'filtered_test_bad.jsonl')"
      ],
      "metadata": {
        "id": "mxneXDyM9cr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_results_to_jsonl(raw_results[0:50], 'filtered_test_1-50.jsonl')"
      ],
      "metadata": {
        "id": "1UMBcjxKmPjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(raw_results_good))\n",
        "print(len(raw_results_bad))"
      ],
      "metadata": {
        "id": "IpAjt0R1lwWC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}